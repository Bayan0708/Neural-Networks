{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7cBBYA7xRpM"
      },
      "source": [
        "# Reimplementación desde cero de una red densa en PyTorch\n",
        "\n",
        "**Objetivo:** replicar el ejemplo de Keras \"from scratch\" usando PyTorch, creando clases mínimas (`NaiveDense`, `NaiveSequential`), un generador de batches, el paso de entrenamiento, el lazo de entrenamiento (`fit`) y la evaluación.\n",
        "\n",
        "> Nota: Para ser fieles al diseño original, la **última capa aplica `softmax`** y la pérdida se calcula **desde probabilidades** como `-log(p_true)`.\n"
      ],
      "id": "G7cBBYA7xRpM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enlace de archivo original para Tensor-flow: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter02_mathematical-building-blocks.ipynb"
      ],
      "metadata": {
        "id": "lscSVoolxayc"
      },
      "id": "lscSVoolxayc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Imports y utilidades"
      ],
      "metadata": {
        "id": "MXultiYtxkZt"
      },
      "id": "MXultiYtxkZt"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_NI9WmqxRpW",
        "outputId": "d25d169a-6772-4b56-93f3-a217f8ceb3e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "torch.manual_seed(42)\n",
        "print('PyTorch version:', torch.__version__)"
      ],
      "id": "Y_NI9WmqxRpW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HePQVq7ExRpZ"
      },
      "source": [
        "## 2) `NaiveDense`: capa densa mínima"
      ],
      "id": "HePQVq7ExRpZ"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Y65h4anxRpa"
      },
      "outputs": [],
      "source": [
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation=None):\n",
        "        self.activation = activation  # e.g., F.relu, softmax callable\n",
        "        # Inicialización uniforme (similar a initializer=\"uniform\")\n",
        "        limit = 0.05\n",
        "        self.W = torch.nn.Parameter(torch.empty(input_size, output_size).uniform_(-limit, limit))\n",
        "        self.b = torch.nn.Parameter(torch.zeros(output_size))\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # inputs: (batch, input_size)\n",
        "        x = inputs @ self.W\n",
        "        x = x + self.b\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ],
      "id": "7Y65h4anxRpa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qh4YFKAxRpb"
      },
      "source": [
        "## 3) `NaiveSequential`: composición de capas"
      ],
      "id": "7qh4YFKAxRpb"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G8rgTTErxRpc"
      },
      "outputs": [],
      "source": [
        "class NaiveSequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params += layer.weights\n",
        "        return params"
      ],
      "id": "G8rgTTErxRpc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNvjvITWxRpd"
      },
      "source": [
        "## 4) Generador de mini-lotes `BatchGenerator`"
      ],
      "id": "HNvjvITWxRpd"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eT03ab7DxRpe"
      },
      "outputs": [],
      "source": [
        "class BatchGenerator:\n",
        "    def __init__(self, images, labels, batch_size=128):\n",
        "        assert images.shape[0] == labels.shape[0]\n",
        "        self.index = 0\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "    def next(self):\n",
        "        images = self.images[self.index : self.index + self.batch_size]\n",
        "        labels = self.labels[self.index : self.index + self.batch_size]\n",
        "        self.index += self.batch_size\n",
        "        return images, labels"
      ],
      "id": "eT03ab7DxRpe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDRrkj6sxRpf"
      },
      "source": [
        "## 5) Pérdida `sparse_categorical_crossentropy` desde **probabilidades**\n",
        "Como la última capa aplica `softmax`, calculamos la pérdida manualmente como `-log(p_true)`."
      ],
      "id": "GDRrkj6sxRpf"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "upxVab_FxRpg"
      },
      "outputs": [],
      "source": [
        "def sparse_categorical_crossentropy_from_probs(probs, true_labels):\n",
        "    # probs: (batch, num_classes) con softmax aplicado\n",
        "    # true_labels: (batch,) enteros en [0..C-1]\n",
        "    eps = 1e-12\n",
        "    gathered = probs[torch.arange(probs.size(0)), true_labels]\n",
        "    loss = -torch.log(gathered + eps)\n",
        "    return loss.mean()"
      ],
      "id": "upxVab_FxRpg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XPG_DifxRpi"
      },
      "source": [
        "## 6) Paso de entrenamiento `one_training_step`\n",
        "Hace: forward → pérdida → backward → update (SGD)."
      ],
      "id": "2XPG_DifxRpi"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vRZt5CU5xRpj"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "# El optimizador va sobre todos los pesos del modelo\n",
        "# (se definirá tras crear el modelo)\n",
        "optimizer = None\n",
        "\n",
        "def update_weights():\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "def one_training_step(model, images_batch, labels_batch):\n",
        "    # forward\n",
        "    predictions = model(images_batch)  # probabilidades (softmax ya aplicado en la última capa)\n",
        "    loss = sparse_categorical_crossentropy_from_probs(predictions, labels_batch)\n",
        "\n",
        "    # backward + update\n",
        "    loss.backward()\n",
        "    update_weights()\n",
        "    return loss.item()"
      ],
      "id": "vRZt5CU5xRpj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIwXzeabxRpj"
      },
      "source": [
        "## 7) Lazo de entrenamiento `fit`"
      ],
      "id": "VIwXzeabxRpj"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p1x87ovzxRpk"
      },
      "outputs": [],
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "    for epoch_counter in range(epochs):\n",
        "        print(f\"Epoch {epoch_counter}\")\n",
        "        batch_generator = BatchGenerator(images, labels, batch_size=batch_size)\n",
        "        batch_generator.index = 0\n",
        "        for batch_counter in range(batch_generator.num_batches):\n",
        "            images_batch, labels_batch = batch_generator.next()\n",
        "            loss = one_training_step(model, images_batch, labels_batch)\n",
        "            if batch_counter % 100 == 0:\n",
        "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
      ],
      "id": "p1x87ovzxRpk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViPTVeIDxRpl"
      },
      "source": [
        "## 8) Cargar MNIST y preparar tensores `(N, 784)` en `[0,1]`"
      ],
      "id": "ViPTVeIDxRpl"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV0XE-zPxRpl",
        "outputId": "91eaf373-16e4-49f6-d8ae-981922c185e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 485kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.85MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 784]),\n",
              " torch.Size([60000]),\n",
              " torch.Size([10000, 784]),\n",
              " torch.Size([10000]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tfms = transforms.ToTensor()  # convierte a float32 en [0,1]\n",
        "train_ds = datasets.MNIST(root='./data', train=True,  download=True, transform=tfms)\n",
        "test_ds  = datasets.MNIST(root='./data', train=False, download=True, transform=tfms)\n",
        "\n",
        "# Construimos tensores grandes (N, 1, 28, 28) y los aplastamos a (N, 784)\n",
        "train_images = torch.stack([train_ds[i][0] for i in range(len(train_ds))]).view(-1, 28*28)\n",
        "train_labels = torch.tensor([train_ds[i][1] for i in range(len(train_ds))]).long()\n",
        "test_images  = torch.stack([test_ds[i][0]  for i in range(len(test_ds))]).view(-1, 28*28)\n",
        "test_labels  = torch.tensor([test_ds[i][1] for i in range(len(test_ds))]).long()\n",
        "\n",
        "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
      ],
      "id": "kV0XE-zPxRpl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Onn8gDpxRpm"
      },
      "source": [
        "## 9) Definir el modelo `NaiveSequential` con `softmax` final y el optimizador SGD"
      ],
      "id": "6Onn8gDpxRpm"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXidS8R2xRpm",
        "outputId": "96c2a342-5d51-4480-8729-4d86467f3d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parámetros totales: 407050\n"
          ]
        }
      ],
      "source": [
        "def softmax_activation(x):\n",
        "    return F.softmax(x, dim=1)\n",
        "\n",
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28*28, output_size=512, activation=F.relu),\n",
        "    NaiveDense(input_size=512,   output_size=10,  activation=softmax_activation),\n",
        "])\n",
        "\n",
        "assert len(model.weights) == 4  # W1,b1,W2,b2\n",
        "\n",
        "# Ahora que el modelo existe, instanciamos el optimizador sobre sus pesos\n",
        "optimizer = torch.optim.SGD(model.weights, lr=learning_rate)\n",
        "print('Parámetros totales:', sum(p.numel() for p in model.weights))"
      ],
      "id": "IXidS8R2xRpm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noP_2A9zxRpm"
      },
      "source": [
        "## 10) Entrenamiento (epochs=10, batch_size=128)"
      ],
      "id": "noP_2A9zxRpm"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwRG9sHsxRpn",
        "outputId": "af8bbaff-c442-4276-c2c7-35eef0566498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "loss at batch 0: 2.32\n",
            "loss at batch 100: 2.30\n",
            "loss at batch 200: 2.24\n",
            "loss at batch 300: 2.21\n",
            "loss at batch 400: 2.17\n",
            "Epoch 1\n",
            "loss at batch 0: 2.14\n",
            "loss at batch 100: 2.14\n",
            "loss at batch 200: 2.07\n",
            "loss at batch 300: 2.04\n",
            "loss at batch 400: 2.00\n",
            "Epoch 2\n",
            "loss at batch 0: 1.96\n",
            "loss at batch 100: 1.98\n",
            "loss at batch 200: 1.89\n",
            "loss at batch 300: 1.86\n",
            "loss at batch 400: 1.82\n",
            "Epoch 3\n",
            "loss at batch 0: 1.77\n",
            "loss at batch 100: 1.80\n",
            "loss at batch 200: 1.69\n",
            "loss at batch 300: 1.67\n",
            "loss at batch 400: 1.63\n",
            "Epoch 4\n",
            "loss at batch 0: 1.57\n",
            "loss at batch 100: 1.62\n",
            "loss at batch 200: 1.49\n",
            "loss at batch 300: 1.48\n",
            "loss at batch 400: 1.46\n",
            "Epoch 5\n",
            "loss at batch 0: 1.38\n",
            "loss at batch 100: 1.45\n",
            "loss at batch 200: 1.30\n",
            "loss at batch 300: 1.31\n",
            "loss at batch 400: 1.31\n",
            "Epoch 6\n",
            "loss at batch 0: 1.22\n",
            "loss at batch 100: 1.29\n",
            "loss at batch 200: 1.14\n",
            "loss at batch 300: 1.16\n",
            "loss at batch 400: 1.18\n",
            "Epoch 7\n",
            "loss at batch 0: 1.08\n",
            "loss at batch 100: 1.16\n",
            "loss at batch 200: 1.00\n",
            "loss at batch 300: 1.04\n",
            "loss at batch 400: 1.07\n",
            "Epoch 8\n",
            "loss at batch 0: 0.97\n",
            "loss at batch 100: 1.04\n",
            "loss at batch 200: 0.90\n",
            "loss at batch 300: 0.94\n",
            "loss at batch 400: 0.98\n",
            "Epoch 9\n",
            "loss at batch 0: 0.88\n",
            "loss at batch 100: 0.95\n",
            "loss at batch 200: 0.81\n",
            "loss at batch 300: 0.86\n",
            "loss at batch 400: 0.92\n"
          ]
        }
      ],
      "source": [
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ],
      "id": "OwRG9sHsxRpn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEbGrbVmxRpn"
      },
      "source": [
        "## 11) Evaluación: exactitud en el conjunto de prueba"
      ],
      "id": "eEbGrbVmxRpn"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4pl_EIOxRpo",
        "outputId": "04e2173b-ad1f-4647-c0e6-8d5f958c1af0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.83\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    probs = model(test_images)                       # ya con softmax\n",
        "    predicted = probs.argmax(dim=1)\n",
        "    matches = (predicted == test_labels)\n",
        "    accuracy = matches.float().mean().item()\n",
        "print(f\"accuracy: {accuracy:.2f}\")"
      ],
      "id": "B4pl_EIOxRpo"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}